{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper.util_ml as uml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import _pickle as pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictor(keys):\n",
    "    predictor_keys = []\n",
    "    for key in keys:\n",
    "        for func, name in zip([np.nanmean, np.nanstd], ['Mean', 'STD']):\n",
    "            predictor_keys.append(name+'_'+key)\n",
    "            if key != 'HR' and name == 'Mean':\n",
    "                predictor_keys.append(name+'_max_Xcorr_HR_'+key)\n",
    "                predictor_keys.append(name +'_min_Xcorr_HR_' + key)\n",
    "    return predictor_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dem_predictors():\n",
    "    dem_keys = ['Mean_CONTINUOUS_Age', 'Mean_CATEGORICAL_Sex', 'Mean_CONTINUOUS_HH', 'Mean_CONTINUOUS_MFS', 'Mean_CONTINUOUS_WFNS', 'Mean_CONTINUOUS_GCS']\n",
    "    return dem_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchordir = \n",
    "get_predictor_data(anchordir,all_predictor_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictor_data(anchordir,all_predictor_keys):\n",
    "    all_predictors = pd.DataFrame()\n",
    "    for file_name in sorted(os.listdir(anchordir)):\n",
    "        # if '2007' in file_name:\n",
    "        if not os.path.isdir(os.path.join(anchordir, file_name)):\n",
    "            print(file_name)\n",
    "            pid = file_name.split('_')[1]\n",
    "            # break\n",
    "            fin = open(os.path.join(anchordir,file_name), 'rb')\n",
    "            patient = pickle.load(fin, encoding='latin1')\n",
    "            fin.close()\n",
    "            ## add predictor vals\n",
    "            pat_predictor_vals = patient[all_predictor_keys]\n",
    "\n",
    "            ## add shop ids vals\n",
    "            pat_predictor_vals['Shopid'] = [pid]  * pat_predictor_vals.shape[0]\n",
    "\n",
    "            #add labels\n",
    "            pat_predictor_vals['label'] = patient['Mean_DCI_Index']\n",
    "\n",
    "            # add hours\n",
    "            pat_predictor_vals['hours'] = np.arange(1, patient.shape[0] + 1, 1)\n",
    "\n",
    "            #append all the predictors in one feature matrix\n",
    "            all_predictors = pd.concat([all_predictors,pat_predictor_vals],ignore_index=True)\n",
    "    return all_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_NN(all_predictors,demfeats_vals,labelsvals,continuous_cols,categorical_cols,all_classifiers,results_file_name,k_feat=60,include_dems=True,going_back_from_dci = False, start_time=24*3):\n",
    "    results_hours = dict()\n",
    "    anchortimepoint = 168 # since we have 14*24 = 336 hours data, anchor will be at 336/2 = 168\n",
    "    # for numofhours in np.arange(12, 336+12, 12):  # -156,156,24 - that ways -12 to 12 will be the full day with DCI\n",
    "    if going_back_from_dci == True:\n",
    "        range_idx = np.flip(np.arange(0, 168, 12))\n",
    "    else:\n",
    "        range_idx = np.arange(start_time, 168+12, 12) # TODO: WE ARE CREATING MODELS GOING FORWARD STARTING FROM DAY=3,24*3 hrs\n",
    "    for numofhours in range_idx:  # -156,156,24 - that ways -12 to 12 will be the full day with DCI\n",
    "        print(numofhours)\n",
    "        if going_back_from_dci:\n",
    "            df_filtered = all_predictors[(all_predictors.hours <= 168) & (\n",
    "                all_predictors.hours > numofhours)]\n",
    "            # np.unique(df_filtered.hours)\n",
    "        else:\n",
    "            df_filtered = all_predictors[(all_predictors.hours >= start_time) & (\n",
    "                all_predictors.hours <= numofhours)]\n",
    "        featurevals = df_filtered[[IDcol] + predictor_keys].groupby(IDcol).agg(\n",
    "            [uml.maxminRange_, np.nanmean, np.nanstd, np.nanmedian, uml.IQR_Range(25, 75),\n",
    "             uml.entropy_])\n",
    "        all_predictors.keys().T\n",
    "        all_predictors.shape\n",
    "        if include_dems:\n",
    "            featurevals = pd.concat([featurevals, demfeats_vals], join='inner', axis=1)\n",
    "            featurevals = featurevals.dropna(axis=0, thresh=35)\n",
    "        else:\n",
    "            featurevals = featurevals.dropna(axis=0, thresh=35)\n",
    "        labelsvals_hours = labelsvals[labelsvals.index.isin(featurevals.index)]\n",
    "        featurevals = featurevals.astype(float)\n",
    "        for classifier_ ,name in zip([nn_classifier],['NN']):\n",
    "            print(name)\n",
    "            compute_riskscore = True\n",
    "            if numofhours >=48 :\n",
    "                compute_riskscore = True\n",
    "            (cnf_mean_run, (mean_auc, std_auc), classifier_, coefvals, probabvals_all_pos_all,\n",
    "             probabvals_all_neg_all, auc_all,hl_all,chi_all) = uml.run_other_models_NN(classifier_,\n",
    "                                                                 x_true=featurevals,\n",
    "                                                                 y_true=labelsvals_hours, cv=cv, name =name,\n",
    "                                                                 continuous_cols= continuous_cols,\n",
    "                                                                 categorical_cols=categorical_cols,labelsvals=labelsvals_hours,predictor_keys = predictor_keys, demo_predictor_keys=demo_predictor_keys,results_file_name=results_file_name,\n",
    "                                                                 fillNa=True, k_feat=k_feat, toplot=False,#k_feat=60\n",
    "                                                                 compute_riskscore=compute_riskscore, dataval=all_predictors,\n",
    "                                                                 include_dems=include_dems)\n",
    "\n",
    "            if numofhours >=48 and compute_riskscore:\n",
    "                a=dict([])\n",
    "                a['pos']  = probabvals_all_pos_all\n",
    "                a['neg'] = probabvals_all_neg_all\n",
    "                pickle.dump(a, open(results_file_name + \"riskscores_\"+name +\"_\"+str(numofhours)+ \"__hourly.p\", \"wb\"))\n",
    "\n",
    "            print(str(np.round(np.median(auc_all), 3)) + ' ' + str(\n",
    "                np.round(np.percentile(auc_all, [25, 75]), 3)))\n",
    "                # print(classifier_)\n",
    "            results_predictor = dict();\n",
    "            results_predictor['Median_Confusion_Matrix'] = np.nanmedian(cnf_mean_run, 0)\n",
    "            results_predictor['Mean_Confusion_Matrix'] = np.nanmean(cnf_mean_run, 0)\n",
    "            results_predictor['std_Confusion_Matrix'] = np.nanstd(cnf_mean_run, 0)\n",
    "            results_predictor['mean_auc'] = mean_auc\n",
    "            results_predictor['std_auc'] = std_auc\n",
    "            results_predictor['N'] = featurevals.shape[0]\n",
    "            results_predictor['N_positive'] = np.sum(labelsvals.values)\n",
    "            results_predictor['Hours'] = (numofhours)\n",
    "            results_predictor['Classifier'] = deepcopy(classifier_)\n",
    "            results_predictor['Coefs'] = (coefvals)\n",
    "            results_predictor['AUCs_all'] = (auc_all)\n",
    "            results_predictor['CM_all'] = (cnf_mean_run)\n",
    "            results_predictor['HL_all'] = (hl_all)\n",
    "            results_predictor['CHI_all'] = (chi_all)\n",
    "            results_hours['All_Feats' + str(numofhours) + '_' + name] = results_predictor\n",
    "    return  results_hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(all_predictors,demfeats_vals,labelsvals,continuous_cols,categorical_cols,all_classifiers,results_file_name,k_feat=60,include_dems=True,going_back_from_dci = False, start_time=24*3):\n",
    "    results_hours = dict()\n",
    "    anchortimepoint = 168 # since we have 14*24 = 336 hours data, anchor will be at 336/2 = 168\n",
    "    # for numofhours in np.arange(12, 336+12, 12):  # -156,156,24 - that ways -12 to 12 will be the full day with DCI\n",
    "    if going_back_from_dci == True:\n",
    "        range_idx = np.flip(np.arange(0, 168, 12))\n",
    "    else:\n",
    "        range_idx = np.arange(start_time, 168+12, 12) # TODO: WE ARE CREATING MODELS GOING FORWARD STARTING FROM DAY=3,24*3 hrs\n",
    "    for numofhours in range_idx:  # -156,156,24 - that ways -12 to 12 will be the full day with DCI\n",
    "        print(numofhours)\n",
    "        # if numofhours !=168:\n",
    "        #     continue\n",
    "        if going_back_from_dci:\n",
    "            df_filtered = all_predictors[(all_predictors.hours  <=  168) & (\n",
    "                all_predictors.hours   > numofhours)]\n",
    "            # np.unique(df_filtered.hours)\n",
    "        else:\n",
    "            df_filtered = all_predictors[(all_predictors.hours >= start_time) & (\n",
    "                all_predictors.hours   <= numofhours)]\n",
    "        featurevals = df_filtered[[IDcol] + predictor_keys].groupby(IDcol).agg(\n",
    "            [uml.maxminRange_, np.nanmean, np.nanstd, np.nanmedian, uml.IQR_Range(25, 75),\n",
    "             uml.entropy_])\n",
    "        all_predictors.keys().T\n",
    "        all_predictors.shape\n",
    "        if include_dems:\n",
    "            featurevals = pd.concat([featurevals, demfeats_vals], join='inner', axis=1)\n",
    "            featurevals = featurevals.dropna(axis=0, thresh=35)\n",
    "        else:\n",
    "            featurevals = featurevals.dropna(axis=0, thresh=35)\n",
    "        labelsvals_hours = labelsvals[labelsvals.index.isin(featurevals.index)]\n",
    "        featurevals = featurevals.astype(float)\n",
    "        for classifier_ ,name in all_classifiers:\n",
    "            if not (name == 'EC' ):\n",
    "                continue\n",
    "            print(name)\n",
    "            compute_riskscore = True\n",
    "            if numofhours >=48 :\n",
    "                compute_riskscore = True\n",
    "            (cnf_mean_run, (mean_auc, std_auc), classifier_, coefvals, probabvals_all_pos_all,\n",
    "             probabvals_all_neg_all, auc_all,hl_all,chi_all) = uml.run_other_models(classifier_,\n",
    "                                                                 x_true=featurevals,\n",
    "                                                                 y_true=labelsvals_hours, cv=cv, name =name,\n",
    "                                                                 continuous_cols= continuous_cols,\n",
    "                                                                 categorical_cols=categorical_cols,labelsvals=labelsvals_hours,predictor_keys = predictor_keys, demo_predictor_keys=demo_predictor_keys,results_file_name=results_file_name,\n",
    "                                                                 fillNa=True, k_feat=k_feat, toplot=False,#k_feat=60\n",
    "                                                                 compute_riskscore=compute_riskscore, dataval=all_predictors,\n",
    "                                                                 include_dems=include_dems)\n",
    "            if numofhours >=48 and compute_riskscore:\n",
    "                a=dict([])\n",
    "                a['pos']  = probabvals_all_pos_all\n",
    "                a['neg'] = probabvals_all_neg_all\n",
    "                pickle.dump(a, open(results_file_name + \"riskscores_\"+name +\"_\"+str(numofhours)+ \"__hourly.p\", \"wb\"))\n",
    "\n",
    "            print(str(np.round(np.median(auc_all), 3)) + ' ' + str(\n",
    "                np.round(np.percentile(auc_all, [25, 75]), 3)))\n",
    "                # print(classifier_)\n",
    "            results_predictor = dict();\n",
    "            results_predictor['Median_Confusion_Matrix'] = np.nanmedian(cnf_mean_run, 0)\n",
    "            results_predictor['Mean_Confusion_Matrix'] = np.nanmean(cnf_mean_run, 0)\n",
    "            results_predictor['std_Confusion_Matrix'] = np.nanstd(cnf_mean_run, 0)\n",
    "            results_predictor['mean_auc'] = mean_auc\n",
    "            results_predictor['std_auc'] = std_auc\n",
    "            results_predictor['N'] = featurevals.shape[0]\n",
    "            results_predictor['N_positive'] = np.sum(labelsvals.values)\n",
    "            results_predictor['Hours'] = (numofhours)\n",
    "            results_predictor['Classifier'] = deepcopy(classifier_)\n",
    "            results_predictor['Coefs'] = (coefvals)\n",
    "            results_predictor['AUCs_all'] = (auc_all)\n",
    "            results_predictor['CM_all'] = (cnf_mean_run)\n",
    "            results_predictor['HL_all'] = (hl_all)\n",
    "            results_predictor['CHI_all'] = (chi_all)\n",
    "            results_hours['All_Feats' + str(numofhours) + '_' + name] = results_predictor\n",
    "    return  results_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_csv(results_hours,results_file_name,going_back_from_dci=True, start_time=24*3,name='NN'):\n",
    "    w = csv.writer(open(results_file_name, \"w\"))\n",
    "    w.writerow(\n",
    "        [' ', 'LR', ' ', ' ', ' ', ' ', ' ', 'SL', ' ', ' ', ' ', ' ', ' ', 'SK', ' ', ' ', ' ', ' ', ' ', 'RF', ' ',\n",
    "         ' ', ' ', ' ', ' ', 'EC', ' ', ' ', ' ', ' ', ' '])\n",
    "    w.writerow(\n",
    "        ['Time from DCI', 'AUC', 'AUC_med', 'TP', 'FP', 'FN', 'TN', 'AUC', 'AUC_med', 'TP', 'FP', 'FN', 'TN', 'AUC',\n",
    "         'AUC_med', 'TP', 'FP', 'FN', 'TN', 'AUC', 'AUC_med', 'TP', 'FP', 'FN', 'TN', 'AUC', 'AUC_med', 'TP', 'FP',\n",
    "         'FN', 'TN'])\n",
    "    # for numofhours in np.arange(-144, 180, 12):\n",
    "    # for numofhours in np.arange(12, 336 + 12, 12):  # -156,156,24 - that ways -12 to 12 will be the full day with DCI\n",
    "    if going_back_from_dci == True:\n",
    "        range_idx = np.flip(np.arange(0, 168, 12))\n",
    "    else:\n",
    "        # range_idx = np.arange(12, 168+12, 12)\n",
    "        range_idx = np.arange(start_time, 168 + 12,\n",
    "                              12)  # TODO: WE ARE CREATING MODELS GOING FORWARD STARTING FROM DAY=3,24*3 hrs\n",
    "    for numofhours in range_idx:  # -156,156,24 - that ways -12 to 12 will be the full day with DCI\n",
    "        print(numofhours)\n",
    "        results_val = list()\n",
    "        results_val.append(str(numofhours / 24))\n",
    "        # for name in ['LR', 'SL', 'SK', 'RF', 'EC']:\n",
    "        for name in [name]:\n",
    "            print(name)\n",
    "            result_predictor = results_hours['All_Feats' + str(numofhours) + '_' + name]\n",
    "            cnf_all = result_predictor['CM_all']\n",
    "            norm_cnf_all = uml.norm_conf_matrices(cnf_all)\n",
    "            cm25, cm, cm75 = np.percentile(norm_cnf_all, [25, 50, 75], axis=0)\n",
    "            AUCs = str(np.round(result_predictor['mean_auc'], 2)) + ' +/- ' + str(\n",
    "                np.round(result_predictor['std_auc'], 3))\n",
    "            AUCs_M = str(np.round(np.median(result_predictor['AUCs_all']), 2)) + ' ' + str(\n",
    "                np.round(np.percentile(result_predictor['AUCs_all'], [25, 75]), 2))\n",
    "            std_cm = result_predictor['std_Confusion_Matrix'] / cm.sum(axis=1)[:, np.newaxis]\n",
    "            TP = str(np.round(cm[0, 0], 2)) + ' [' + str(np.round(cm25[0, 0], 2)) + '-' + str(\n",
    "                np.round(cm75[0, 0], 2)) + ']'\n",
    "            FP = str(np.round(cm[0, 1], 2)) + ' [' + str(np.round(cm25[0, 1], 2)) + '-' + str(\n",
    "                np.round(cm75[0, 1], 2)) + ']'\n",
    "            FN = str(np.round(cm[1, 0], 2)) + ' [' + str(np.round(cm25[1, 0], 2)) + '-' + str(\n",
    "                np.round(cm75[1, 0], 2)) + ']'\n",
    "            TN = str(np.round(cm[1, 1], 2)) + ' [' + str(np.round(cm25[1, 1], 2)) + '-' + str(\n",
    "                np.round(cm75[1, 1], 2)) + ']'\n",
    "            results_val.append(AUCs)\n",
    "            results_val.append(AUCs_M)\n",
    "            results_val.append(TP)\n",
    "            results_val.append(FP)\n",
    "            results_val.append(FN)\n",
    "            results_val.append(TN)\n",
    "        print(results_val)\n",
    "        w.writerow(results_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtick_label_vals_feats(include_dems=True):\n",
    "    xticklabelsvals = [\n",
    "\n",
    "        # # ICP\n",
    "        'ICP Mean R',\n",
    "        'ICP Mean M',\n",
    "        'ICP Mean S',\n",
    "        'ICP Mean Med',\n",
    "        'ICP Mean IQRR',\n",
    "        'ICP Mean E',\n",
    "        'ICP SD R',\n",
    "        'ICP SD M',\n",
    "        'ICP SD S',\n",
    "        'ICP SD Med',\n",
    "        'ICP SD IQRR',\n",
    "        'ICP SD E',\n",
    "        'Xcorr HR-ICP R',\n",
    "        'Xcorr HR-ICP M',\n",
    "        'Xcorr HR-ICP S',\n",
    "        'Xcorr HR-ICP Med',\n",
    "        'Xcorr HR-ICP IQRR',\n",
    "        'Xcorr HR-ICP E',  \n",
    "        # RR\n",
    "        'RR Mean R',\n",
    "        'RR Mean M',\n",
    "        'RR Mean S',\n",
    "        'RR Mean Med',\n",
    "        'RR Mean IQRR',\n",
    "        'RR Mean E',\n",
    "        'RR SD R',\n",
    "        'RR SD M',\n",
    "        'RR SD S',\n",
    "        'RR SD Med',\n",
    "        'RR SD IQRR',\n",
    "        'RR SD E',\n",
    "        'Xcorr HR-RR R',\n",
    "        'Xcorr HR-RR M',\n",
    "        'Xcorr HR-RR S',\n",
    "        'Xcorr HR-RR Med',\n",
    "        'Xcorr HR-RR IQRR',\n",
    "        'Xcorr HR-RR E',\n",
    "\n",
    "        # 'SPO2\n",
    "        'SPO2 Mean R',\n",
    "        'SPO2 Mean M',\n",
    "        'SPO2 Mean S',\n",
    "        'SPO2 Mean Med',\n",
    "        'SPO2 Mean IQRR',\n",
    "        'SPO2 Mean E',\n",
    "        'SPO2 SD R',\n",
    "        'SPO2 SD M',\n",
    "        'SPO2 SD S',\n",
    "        'SPO2 SD Med',\n",
    "        'SPO2 SD IQRR',\n",
    "        'SPO2 SD E',\n",
    "        'Xcorr HR-SPO2 R',\n",
    "        'Xcorr HR-SPO2 M',\n",
    "        'Xcorr HR-SPO2 S',\n",
    "        'Xcorr HR-SPO2 Med',\n",
    "        'Xcorr HR-SPO2 IQRR',\n",
    "        'Xcorr HR-SPO2 E',\n",
    "\n",
    "        # ARD\n",
    "        'ARD Mean R',\n",
    "        'ARD Mean M',\n",
    "        'ARD Mean S',\n",
    "        'ARD Mean Med',\n",
    "        'ARD Mean IQRR',\n",
    "        'ARD Mean E',\n",
    "        'ARD SD R',\n",
    "        'ARD SD M',\n",
    "        'ARD SD S',\n",
    "        'ARD SD Med',\n",
    "        'ARD SD IQRR',\n",
    "        'ARD SD E',\n",
    "        'Xcorr HR-ARD R',\n",
    "        'Xcorr HR-ARD M',\n",
    "        'Xcorr HR-ARD S',\n",
    "        'Xcorr HR-ARD Med',\n",
    "        'Xcorr HR-ARD IQRR',\n",
    "        'Xcorr HR-ARD E',\n",
    "\n",
    "        # ARS\n",
    "        'ARS Mean R',\n",
    "        'ARS Mean M',\n",
    "        'ARS Mean S',\n",
    "        'ARS Mean Med',\n",
    "        'ARS Mean IQRR',\n",
    "        'ARS Mean E',\n",
    "        'ARS SD R',\n",
    "        'ARS SD M',\n",
    "        'ARS SD S',\n",
    "        'ARS SD Med',\n",
    "        'ARS SD IQRR',\n",
    "        'ARS SD E',\n",
    "        'Xcorr HR-ARS R',\n",
    "        'Xcorr HR-ARS M',\n",
    "        'Xcorr HR-ARS S',\n",
    "        'Xcorr HR-ARS Med',\n",
    "        'Xcorr HR-ARS IQRR',\n",
    "        'Xcorr HR-ARS E',\n",
    "\n",
    "        # # # TEMP\n",
    "        'TEMP Mean R',\n",
    "        'TEMP Mean M',\n",
    "        'TEMP Mean S',\n",
    "        'TEMP Mean Med',\n",
    "        'TEMP Mean IQRR',\n",
    "        'TEMP Mean E',\n",
    "        'TEMP SD R',\n",
    "        'TEMP SD M',\n",
    "        'TEMP SD S',\n",
    "        'TEMP SD Med',\n",
    "        'TEMP SD IQRR',\n",
    "        'TEMP SD E',\n",
    "        'Xcorr HR-TEMP R',\n",
    "        'Xcorr HR-TEMP M',\n",
    "        'Xcorr HR-TEMP S',\n",
    "        'Xcorr HR-TEMP Med',\n",
    "        'Xcorr HR-TEMP IQRR',\n",
    "        'Xcorr HR-TEMP E',\n",
    "\n",
    "        'HR Mean R',\n",
    "        'HR Mean M',\n",
    "        'HR Mean S',\n",
    "        'HR Mean Med',\n",
    "        'HR Mean IQRR',\n",
    "        'HR Mean E',\n",
    "        'HR SD R',\n",
    "        'HR SD M',\n",
    "        'HR SD S',\n",
    "        'HR SD Med',\n",
    "        'HR SD IQRR',\n",
    "        'HR SD E'\n",
    "    ]\n",
    "    if include_dems:\n",
    "        xticklabelsvals.extend(['Age',\n",
    "        'Sex',\n",
    "        'HH_Adm',\n",
    "        'MFS',\n",
    "        'WFNS',\n",
    "        'GCS'])\n",
    "    return  xticklabelsvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_hours,xticklabelsvals,all_predictors,file_name):\n",
    "    for color, name in [\n",
    "        ('b', 'EC'),\n",
    "        ('g', 'SL'),\n",
    "        ('r', 'LR'),\n",
    "        ('c', 'SK'),\n",
    "        ('m', 'RF')\n",
    "    ]:\n",
    "        print(name)\n",
    "        mean_auc_vals = ()\n",
    "        std_auc_vals = ()\n",
    "        time_vals = ()\n",
    "        classifier_all = ()\n",
    "        coefval_time = ()\n",
    "        coefval_time2 = ()\n",
    "        counter = 1\n",
    "        for p_id, p_info in results_hours.items():\n",
    "            if name in p_id:\n",
    "                print(\"\\n\", p_info['N'], '\\t', p_info['N_positive'], '\\t', p_id, '\\t AUC \\t', p_info['mean_auc'],\n",
    "                      '\\t +/- \\t', p_info['std_auc'])\n",
    "                p_info['HL_all']\n",
    "                p_info['CHI_all']\n",
    "                np.round(np.median(p_info['AUCs_all']), 3)\n",
    "                mean_auc_vals = np.append(mean_auc_vals, np.median(p_info['AUCs_all']))\n",
    "                std_auc_vals = np.append(std_auc_vals, p_info['std_auc'])\n",
    "                time_vals = np.append(time_vals, p_info['Hours'])\n",
    "                classifier_all = np.append(classifier_all, p_info['Classifier'])\n",
    "                b = np.where(p_info['Classifier'].get_params()['feature_selection'].get_support())\n",
    "                xticklabelsvals1 = np.array(xticklabelsvals)\n",
    "                # print(xticklabelsvals1[b])\n",
    "                if counter == 1:\n",
    "                    b = np.where(p_info['Classifier'].get_params()[\n",
    "                                     'feature_selection'].get_support())  # since only k best are choosen we will have to assign those to correct locations\n",
    "                    coefval_time = np.nan * np.zeros(250)  ## TODO: Get this dynamically\n",
    "                    if p_info['Coefs'].shape[1] > 0:\n",
    "                        coefval_time[b] = np.mean(p_info['Coefs'], axis=0)\n",
    "                    counter = counter + 1;\n",
    "                else:\n",
    "                    if np.mean(p_info['Coefs'], axis=0).shape[0] == 0:\n",
    "                        continue\n",
    "                    b = np.where(p_info['Classifier'].get_params()[\n",
    "                                     'feature_selection'].get_support())  # since only k best are choosen we will have to assign those to correct locations\n",
    "                    coefval_temp = np.nan * np.zeros(250) ## TODO: Get this dynamically\n",
    "                    if p_info['Coefs'].shape[1] > 0:\n",
    "                        coefval_temp[b] = np.mean(p_info['Coefs'], axis=0)\n",
    "                    coefval_time = np.vstack([coefval_time, coefval_temp]);\n",
    "                    # coefval_time2 = np.vstack([coefval_time2, np.squeeze(p_info['Classifier']._final_estimator.coef_)]);\n",
    "        sortidx = np.unravel_index(np.argsort(time_vals, axis=None), time_vals.shape)\n",
    "        sortetimeval = time_vals[sortidx] / 24 -7\n",
    "        plt.errorbar(sortetimeval, mean_auc_vals[sortidx], std_auc_vals[sortidx], linewidth=2.5, color=color,\n",
    "                     elinewidth=0.5, label=name, fmt='')\n",
    "        plt.axhline(y=0.5, color='k', linestyle='--', linewidth=1.5)\n",
    "        plt.axvline(x=0, color='k', linestyle='-', linewidth=2.5)\n",
    "        sortetimeval = np.asarray(sortetimeval, dtype='str')\n",
    "        sortetimeval[sortetimeval == '0.0'] = 'DCI'\n",
    "        plt.yticks(fontsize=12, fontweight='bold');\n",
    "        plt.xticks(time_vals[sortidx]/24-7,sortetimeval,fontsize=13,fontweight='bold',rotation=45)\n",
    "        # plt.xticks(np.arange(-7, 8, 1), np.arange(-7, 8, 1), fontsize=13, fontweight='bold', rotation=45)\n",
    "        plt.ylim([0.4, 1.00])\n",
    "        # plt.xlim([-6.5, 7.5])\n",
    "        plt.xlabel('Time(days)', fontsize=13, fontweight='bold')\n",
    "        plt.ylabel('AUCs', fontsize=13, fontweight='bold')\n",
    "        plt.text(0.51, 0.51, 'Chance level', fontsize=13, fontweight='bold')\n",
    "        plt.legend(prop={'size': 10, 'weight': 'bold'})\n",
    "        # plt.show()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(file_name+name+'.png')\n",
    "        plt.savefig(file_name + name + '.pdf')\n",
    "        plt.close()\n",
    "        # if coefval_time.shape[1] != 0:\n",
    "        if len(coefval_time.shape) > 1:\n",
    "            coefval_time = coefval_time[np.squeeze(np.asarray(sortidx)), :]  # sort it based on the time index\n",
    "            plt.figure(figsize=(7.30, 7.69))\n",
    "            bounds = np.array([-1, -0.5, -0.125, 0, 0.125, 0.5, 1])\n",
    "            norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256)\n",
    "            norm = colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n",
    "                                     vmin=-1.0, vmax=1.0)\n",
    "            plt.imshow(nanzscore(np.transpose(coefval_time)), interpolation=\"none\",\n",
    "                       extent=[-6, 7, coefval_time.shape[1], 0], cmap=plt.get_cmap('PiYG'),\n",
    "                       # norm=norm\n",
    "                       vmin=-3.5, vmax=3.5\n",
    "                       );\n",
    "            plt.colorbar()\n",
    "            ax = plt.gca()\n",
    "            ax.set_aspect(0.25)\n",
    "            plt.xlabel('Time(days)', fontsize=13, fontweight='bold')\n",
    "            plt.ylabel('Feature (weights)', fontsize=13, fontweight='bold')\n",
    "            plt.axvline(x=0, color='k', linestyle='-', linewidth=2.5)\n",
    "            plt.yticks(np.arange(0, coefval_time.shape[1], 1), xticklabelsvals, fontsize=6, rotation=15)\n",
    "            plt.axvline(x=0, color='k', linestyle='-', linewidth=2.5)\n",
    "            # plt.show()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(file_name+'_feats_'+name+'.png')\n",
    "            plt.savefig(file_name + '_feats_' + name + '.pdf')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_risk_scores(risk_score_file_name,name,vmin=.25,vmax=.50):\n",
    "    fin = open(risk_score_file_name, 'rb')\n",
    "    a = pickle.load(fin, encoding='latin1')\n",
    "    fin.close()\n",
    "    timevals = np.arange(12, 168 + 12, 12) - 168\n",
    "    timevals = np.arange(-168, 0, 1)\n",
    "    pos_all = a['pos']\n",
    "    neg_all = a['neg']\n",
    "    if len(pos_all) == 0:\n",
    "        return\n",
    "    len(pos_all)\n",
    "    for i in np.arange(0,5):\n",
    "        print(len(pos_all[i]))\n",
    "    probabvals_all_pos = np.concatenate(np.array(pos_all))\n",
    "    probabvals_all_neg = np.concatenate(np.array(neg_all))\n",
    "    med_rs = np.nanmedian(probabvals_all_pos, axis=0)\n",
    "    q75, q25 = np.nanpercentile(probabvals_all_pos, [75, 25], axis=0)\n",
    "    print('Risk scores median (IQR) pos : ', np.around(med_rs[-1], 2), '(', np.around(q25[-1], 2), '-',\n",
    "          np.around(q75[-1], 2), ')')\n",
    "    med_rs = np.nanmedian(probabvals_all_neg, axis=0)\n",
    "    q75, q25 = np.nanpercentile(probabvals_all_neg, [75, 25], axis=0)\n",
    "    print('Risk scores median (IQR) pos : ', np.around(med_rs[-1], 2), '(', np.around(q25[-1], 2), '-',\n",
    "          np.around(q75[-1], 2), ')')\n",
    "    mean_plot = True\n",
    "    probabvals_all = np.concatenate([probabvals_all_neg,probabvals_all_pos],axis=0)\n",
    "    plt.figure()\n",
    "    plt.imshow((probabvals_all), cmap=plt.get_cmap('coolwarm'), zorder=1,vmin=vmin,vmax=vmax)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect(0.5)\n",
    "    plt.axhline(y=len(probabvals_all_neg), color='k', linestyle='-', linewidth=2.5)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.title(risk_score_file_name,fontsize=13, fontweight='bold')\n",
    "    plt.xticks(np.arange(0,len(timevals),2),timevals[0::2])\n",
    "\n",
    "    plt.figure()\n",
    "    if mean_plot:\n",
    "        plt.plot(timevals / 24, np.nanmean(probabvals_all_neg, axis=0), linewidth=2.5, color='g')\n",
    "        plt.plot(timevals / 24, np.nanmean(probabvals_all_pos, axis=0), linewidth=2.5, color='r')\n",
    "        plt.fill_between(timevals / 24, np.nanmean(probabvals_all_neg, axis=0) - np.nanstd(probabvals_all_neg, axis=0),\n",
    "                         np.nanmean(probabvals_all_neg, axis=0) + np.nanstd(probabvals_all_neg, axis=0), alpha=0.05,\n",
    "                         edgecolor='g', facecolor='g')\n",
    "        plt.fill_between(timevals / 24, np.nanmean(probabvals_all_pos, axis=0) - np.nanstd(probabvals_all_pos, axis=0),\n",
    "                         np.nanmean(probabvals_all_pos, axis=0) + np.nanstd(probabvals_all_pos, axis=0), alpha=0.05,\n",
    "                         edgecolor='r', facecolor='r')\n",
    "        plt.plot(timevals / 24, np.nanmean(probabvals_all_neg, axis=0) - np.nanstd(probabvals_all_neg, axis=0),\n",
    "                 color='g', alpha=0.1)\n",
    "        plt.plot(timevals / 24, np.nanmean(probabvals_all_neg, axis=0) + np.nanstd(probabvals_all_neg, axis=0),\n",
    "                 color='g', alpha=0.1)\n",
    "        plt.plot(timevals / 24, np.nanmean(probabvals_all_pos, axis=0) - np.nanstd(probabvals_all_pos, axis=0),\n",
    "                 color='r', alpha=0.1)\n",
    "        plt.plot(timevals / 24, np.nanmean(probabvals_all_pos, axis=0) + np.nanstd(probabvals_all_pos, axis=0),\n",
    "                 color='r', alpha=0.1)\n",
    "    else:\n",
    "        plt.plot(timevals / 24, np.nanmedian(probabvals_all_neg, axis=0), linewidth=2.5, color='g')\n",
    "        plt.plot(timevals / 24, np.nanmedian(probabvals_all_pos, axis=0), linewidth=2.5, color='r')\n",
    "        plt.fill_between(timevals / 24, np.nanpercentile(probabvals_all_neg, [25], axis=0)[0],\n",
    "                         np.nanpercentile(probabvals_all_neg, [75], axis=0)[0], alpha=0.05,\n",
    "                         edgecolor='g', facecolor='g')\n",
    "        plt.fill_between(timevals / 24, np.nanpercentile(probabvals_all_pos, [25], axis=0)[0],\n",
    "                         np.nanpercentile(probabvals_all_pos, [75], axis=0)[0], alpha=0.05,\n",
    "                         edgecolor='r', facecolor='r')\n",
    "        plt.plot(timevals / 24, np.nanpercentile(probabvals_all_neg, [25], axis=0)[0], color='g', alpha=0.1)\n",
    "        plt.plot(timevals / 24, np.nanpercentile(probabvals_all_neg, [75], axis=0)[0], color='g', alpha=0.1)\n",
    "        plt.plot(timevals / 24, np.nanpercentile(probabvals_all_pos, [75], axis=0)[0], color='r', alpha=0.1)\n",
    "        plt.plot(timevals / 24, np.nanpercentile(probabvals_all_pos, [25], axis=0)[0], color='r', alpha=0.1)\n",
    "    plt.xlabel('Time(days from Anchor)', fontsize=13, fontweight='bold')\n",
    "    plt.ylabel('Risk Score', fontsize=13, fontweight='bold')\n",
    "    plt.axhline(y=0.35, color='k', linestyle='--', linewidth=1.5)\n",
    "    # plt.axhline(y=0.5, color='k', linestyle='--', linewidth=1.5)\n",
    "    plt.yticks(fontsize=13, fontweight='bold')\n",
    "    plt.xticks((-6, -5, -4, -3, -2, -1, 0), (-6, -5, -4, -3, -2, -1, 'Anchor'), fontsize=13, fontweight='bold',\n",
    "               rotation=45)\n",
    "    plt.title(risk_score_file_name,fontsize=13, fontweight='bold')\n",
    "    plt.legend(('DCI-', 'DCI+'), loc=2, prop=dict(size=13, weight='bold'))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tpv_fpv_over_time(risk_score_file_name):\n",
    "    fin = open(risk_score_file_name, 'rb')\n",
    "    a = pickle.load(fin, encoding='latin1')\n",
    "    fin.close()\n",
    "    timevals = np.arange(12, 168 + 12, 12) - 168\n",
    "    pos_all = a['pos']\n",
    "    neg_all = a['neg']\n",
    "    # pos_all = a[0]\n",
    "    # neg_all = a[1]\n",
    "    threshold_val = 0.35\n",
    "    len(pos_all)\n",
    "    for i in np.arange(0, 5):\n",
    "        print(len(pos_all[i]))\n",
    "    probabvals_all_pos = np.concatenate(np.array(pos_all))\n",
    "    probabvals_all_neg = np.concatenate(np.array(neg_all))\n",
    "    med_rs = np.nanmedian(probabvals_all_pos, axis=0)\n",
    "    q75, q25 = np.nanpercentile(probabvals_all_pos, [75, 25], axis=0)\n",
    "    print('Risk scores median (IQR) pos : ', np.around(med_rs[-1], 2), '(', np.around(q25[-1], 2), '-',\n",
    "          np.around(q75[-1], 2), ')')\n",
    "    threshold_val = np.around(med_rs[-1], 2)\n",
    "    med_rs = np.nanmedian(probabvals_all_neg, axis=0)\n",
    "    q75, q25 = np.nanpercentile(probabvals_all_neg, [75, 25], axis=0)\n",
    "    print('Risk scores median (IQR) pos : ', np.around(med_rs[-1], 2), '(', np.around(q25[-1], 2), '-',\n",
    "          np.around(q75[-1], 2), ')')\n",
    "    mean_plot = True\n",
    "    threshold_val = threshold_val + np.around(med_rs[-1], 2)\n",
    "    threshold_val = threshold_val / 2\n",
    "    propb_neg_pd = pd.DataFrame(probabvals_all_neg)\n",
    "    propb_pos_pd = pd.DataFrame(probabvals_all_pos)\n",
    "    temp = [propb_neg_pd < threshold_val]\n",
    "    discharge_correctly = [np.sum(temp[0][i]) for i in temp[0]]\n",
    "    temp = [propb_pos_pd < threshold_val]\n",
    "    discharge_incorrectly = [np.sum(temp[0][i]) for i in temp[0]]\n",
    "    npv_over_time = [TN / (TN + FN) for (TN, FN) in zip(discharge_correctly, discharge_incorrectly)]\n",
    "    specificity = [number / propb_neg_pd.shape[0] for number in discharge_correctly]\n",
    "    temp = [propb_neg_pd > threshold_val]\n",
    "    dci_incorrectly = [np.sum(temp[0][i]) for i in temp[0]]\n",
    "    temp = [propb_pos_pd > threshold_val]\n",
    "    dci_correctly = [np.sum(temp[0][i]) for i in temp[0]]\n",
    "    ppv_over_time = [TP / (TP + FP) for (TP, FP) in zip(dci_correctly, dci_incorrectly)]\n",
    "    sensitivity = [number / propb_pos_pd.shape[0] for number in dci_correctly]\n",
    "    plt.figure(1)\n",
    "    plt.subplot(3, 2, 1)\n",
    "    # plt.bar(np.arange(0,len(npv_over_time),1)+w,npv_over_time,width=0.1,label=risk_score_file_name)\n",
    "    plt.plot(npv_over_time, label=risk_score_file_name)\n",
    "    plt.title('NPV = TN/(TN+FN)')\n",
    "\n",
    "    # plt.legend()\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot(specificity, label=risk_score_file_name)\n",
    "    plt.title('Specificity - Discharge Correctly (TN)/Total number of negative patients')\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.plot(ppv_over_time, label=risk_score_file_name)\n",
    "    plt.title('PPV = TP/(TP+FP)')\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(sensitivity, label=risk_score_file_name)\n",
    "    plt.title('sensitivity - DCI Predicted(TP)/Total number of DCI patients')\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.plot(1, label=risk_score_file_name)\n",
    "    plt.legend()\n",
    "    # plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_all(univ_type='all',include_dems = True):\n",
    "    target = 'label'\n",
    "    IDcol = 'Shopid'\n",
    "    hourscol = 'hours'\n",
    "    ## load all the pickles and append it in one array\n",
    "    keys = ['HR', 'AR-M', 'AR-D', 'AR-S', 'SPO2', 'RR']\n",
    "    predictor_keys = get_predictor(keys)\n",
    "    demo_predictor_keys = get_dem_predictors()\n",
    "    if include_dems:\n",
    "        # get all the predictor keys\n",
    "        all_predictor_keys = predictor_keys + demo_predictor_keys\n",
    "    else:\n",
    "        all_predictor_keys = predictor_keys\n",
    "    all_predictors_univs_combined = pd.DataFrame([])\n",
    "    labelsvals_univs_combined = pd.DataFrame([])\n",
    "    demfeats_vals_univs_combined = pd.DataFrame([])\n",
    "    if univ_type == 'all':\n",
    "        var_list = ['CUMC','Aachen','UTH']\n",
    "    else:\n",
    "        var_list = [univ_type]\n",
    "    for univ in var_list : # create separate models for each university, and also with the combined data\n",
    "        anchordir = '//prometheus.neuro.columbia.edu//neurocriticalcare//data//Projects//33_Federated_Learning_data//anchor//' + univ + '//'\n",
    "        # ***********************2 . Get all the predictors*******************\n",
    "        all_predictors = get_predictor_data(anchordir, all_predictor_keys)\n",
    "        all_predictors = all_predictors.astype(float)\n",
    "        temp = all_predictors.groupby(['Shopid']).ffill()  # forward fill the missing values\n",
    "        all_predictors[temp.keys()] = temp\n",
    "\n",
    "        # NO NEED TO AGGR WE CAN TAKE UNIQUE TOO\n",
    "        labelsvals = all_predictors[[IDcol, target]].groupby(IDcol).aggregate(\n",
    "            np.nanmedian)  # get median and standard deviation\n",
    "        shopid = all_predictors[[IDcol, target]].groupby(IDcol).groups.keys()\n",
    "        if include_dems:\n",
    "            demfeats_vals = all_predictors[[IDcol] + demo_predictor_keys].groupby(IDcol).aggregate(\n",
    "                [np.nanmedian])  # get median and standard deviation\n",
    "        else:\n",
    "            demfeats_vals = []\n",
    "        all_predictors_univs_combined = pd.concat([all_predictors_univs_combined, all_predictors])\n",
    "        labelsvals_univs_combined = pd.concat([labelsvals_univs_combined, labelsvals])\n",
    "        demfeats_vals_univs_combined = pd.concat([demfeats_vals_univs_combined, demfeats_vals])\n",
    "    return all_predictors_univs_combined,labelsvals_univs_combined,demfeats_vals_univs_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_2003_anchor.pickle\n",
      "patient_2007_anchor.pickle\n",
      "patient_2010_anchor.pickle\n",
      "patient_2011_anchor.pickle\n",
      "patient_2014_anchor.pickle\n",
      "patient_2019_anchor.pickle\n",
      "patient_2020_anchor.pickle\n",
      "patient_2022_anchor.pickle\n",
      "patient_2024_anchor.pickle\n",
      "patient_2026_anchor.pickle\n",
      "patient_2027_anchor.pickle\n",
      "patient_2029_anchor.pickle\n",
      "patient_2030_anchor.pickle\n",
      "patient_2032_anchor.pickle\n",
      "patient_2034_anchor.pickle\n",
      "patient_2035_anchor.pickle\n",
      "patient_2036_anchor.pickle\n",
      "patient_2042_anchor.pickle\n",
      "patient_2043_anchor.pickle\n",
      "patient_2044_anchor.pickle\n",
      "patient_2048_anchor.pickle\n",
      "patient_2050_anchor.pickle\n",
      "patient_2051_anchor.pickle\n",
      "patient_2053_anchor.pickle\n",
      "patient_2054_anchor.pickle\n",
      "patient_2057_anchor.pickle\n",
      "patient_2058_anchor.pickle\n",
      "patient_2060_anchor.pickle\n",
      "patient_2061_anchor.pickle\n",
      "patient_2062_anchor.pickle\n",
      "patient_2064_anchor.pickle\n",
      "patient_2068_anchor.pickle\n",
      "patient_2069_anchor.pickle\n",
      "patient_2070_anchor.pickle\n",
      "patient_2071_anchor.pickle\n",
      "patient_2076_anchor.pickle\n",
      "patient_2077_anchor.pickle\n",
      "patient_2079_anchor.pickle\n",
      "patient_2081_anchor.pickle\n",
      "patient_2083_anchor.pickle\n",
      "patient_2085_anchor.pickle\n",
      "patient_2087_anchor.pickle\n",
      "patient_2088_anchor.pickle\n",
      "patient_2090_anchor.pickle\n",
      "patient_2092_anchor.pickle\n",
      "patient_2093_anchor.pickle\n",
      "patient_2094_anchor.pickle\n",
      "patient_2095_anchor.pickle\n",
      "patient_2096_anchor.pickle\n",
      "patient_2098_anchor.pickle\n",
      "patient_2099_anchor.pickle\n",
      "patient_2101_anchor.pickle\n",
      "patient_2103_anchor.pickle\n",
      "patient_2105_anchor.pickle\n",
      "patient_2107_anchor.pickle\n",
      "patient_2109_anchor.pickle\n",
      "patient_2110_anchor.pickle\n",
      "patient_2111_anchor.pickle\n",
      "patient_2112_anchor.pickle\n",
      "patient_2115_anchor.pickle\n",
      "patient_2116_anchor.pickle\n",
      "patient_2119_anchor.pickle\n",
      "patient_2120_anchor.pickle\n",
      "patient_2121_anchor.pickle\n",
      "patient_2124_anchor.pickle\n",
      "patient_2127_anchor.pickle\n",
      "patient_2130_anchor.pickle\n",
      "patient_2132_anchor.pickle\n",
      "patient_2136_anchor.pickle\n",
      "patient_2143_anchor.pickle\n",
      "patient_2144_anchor.pickle\n",
      "patient_2147_anchor.pickle\n",
      "patient_2151_anchor.pickle\n",
      "patient_2152_anchor.pickle\n",
      "patient_2154_anchor.pickle\n",
      "patient_2157_anchor.pickle\n",
      "patient_2169_anchor.pickle\n",
      "patient_2193_anchor.pickle\n",
      "patient_2197_anchor.pickle\n",
      "patient_2199_anchor.pickle\n",
      "patient_2211_anchor.pickle\n",
      "patient_2221_anchor.pickle\n",
      "patient_2223_anchor.pickle\n",
      "patient_2225_anchor.pickle\n",
      "patient_2227_anchor.pickle\n",
      "patient_2230_anchor.pickle\n",
      "patient_2233_anchor.pickle\n",
      "patient_2235_anchor.pickle\n",
      "patient_2239_anchor.pickle\n",
      "patient_2240_anchor.pickle\n",
      "patient_2244_anchor.pickle\n",
      "patient_2247_anchor.pickle\n",
      "patient_2248_anchor.pickle\n",
      "patient_2249_anchor.pickle\n",
      "patient_2253_anchor.pickle\n",
      "patient_2254_anchor.pickle\n",
      "patient_2256_anchor.pickle\n",
      "patient_2257_anchor.pickle\n",
      "patient_2258_anchor.pickle\n",
      "patient_2259_anchor.pickle\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dci_fl\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\ae2722\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_17180\\\\1197019819.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m IDcol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShopid\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m hourscol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhours\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m all_predictors, labelsvals, demfeats_vals \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43muniv_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCUMC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m predictor_keys \u001b[38;5;241m=\u001b[39m get_predictor(keys)\n\u001b[0;32m      7\u001b[0m demo_predictor_keys \u001b[38;5;241m=\u001b[39m get_dem_predictors()\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mload_data_all\u001b[1;34m(univ_type, include_dems)\u001b[0m\n\u001b[0;32m     22\u001b[0m anchordir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//prometheus.neuro.columbia.edu//neurocriticalcare//data//Projects//33_Federated_Learning_data//anchor//\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m univ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# ***********************2 . Get all the predictors*******************\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m all_predictors \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictor_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchordir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_predictor_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m all_predictors \u001b[38;5;241m=\u001b[39m all_predictors\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     26\u001b[0m temp \u001b[38;5;241m=\u001b[39m all_predictors\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShopid\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mffill()  \u001b[38;5;66;03m# forward fill the missing values\u001b[39;00m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mget_predictor_data\u001b[1;34m(anchordir, all_predictor_keys)\u001b[0m\n\u001b[0;32m     13\u001b[0m pat_predictor_vals \u001b[38;5;241m=\u001b[39m patient[all_predictor_keys]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m## add shop ids vals\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m pat_predictor_vals[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShopid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [pid]  \u001b[38;5;241m*\u001b[39m pat_predictor_vals\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#add labels\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pat_predictor_vals[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m patient[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean_DCI_Index\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dci_fl\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dci_fl\\lib\\site-packages\\pandas\\core\\frame.py:3845\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3842\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(existing_piece, DataFrame):\n\u001b[0;32m   3843\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m-> 3845\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dci_fl\\lib\\site-packages\\pandas\\core\\frame.py:3810\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;66;03m# check if we are modifying a copy\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m \u001b[38;5;66;03m# try to set first as we want an invalid\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;66;03m# value exception to occur first\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 3810\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_setitem_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dci_fl\\lib\\site-packages\\pandas\\core\\generic.py:4018\u001b[0m, in \u001b[0;36mNDFrame._check_setitem_copy\u001b[1;34m(self, t, force)\u001b[0m\n\u001b[0;32m   4016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m com\u001b[38;5;241m.\u001b[39mSettingWithCopyError(t)\n\u001b[0;32m   4017\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 4018\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(t, com\u001b[38;5;241m.\u001b[39mSettingWithCopyWarning, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[43mfind_stack_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dci_fl\\lib\\site-packages\\pandas\\util\\_exceptions.py:32\u001b[0m, in \u001b[0;36mfind_stack_level\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_stack_level\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    Find the first place in the stack that is not inside pandas\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    (tests notwithstanding).\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     stack \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     pkg_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(pd\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dci_fl\\lib\\inspect.py:1678\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(context)\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(context\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1677\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetouterframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dci_fl\\lib\\inspect.py:1655\u001b[0m, in \u001b[0;36mgetouterframes\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1653\u001b[0m framelist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1654\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame:\n\u001b[1;32m-> 1655\u001b[0m     frameinfo \u001b[38;5;241m=\u001b[39m (frame,) \u001b[38;5;241m+\u001b[39m \u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1656\u001b[0m     framelist\u001b[38;5;241m.\u001b[39mappend(FrameInfo(\u001b[38;5;241m*\u001b[39mframeinfo))\n\u001b[0;32m   1657\u001b[0m     frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dci_fl\\lib\\inspect.py:1625\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isframe(frame):\n\u001b[0;32m   1623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is not a frame or traceback object\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(frame))\n\u001b[1;32m-> 1625\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m getfile(frame)\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1627\u001b[0m     start \u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m context\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dci_fl\\lib\\inspect.py:826\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(filename\u001b[38;5;241m.\u001b[39mendswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    824\u001b[0m              importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[0;32m    828\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dci_fl\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "keys = ['HR', 'AR-M', 'AR-D', 'AR-S', 'SPO2', 'RR']\n",
    "target = 'label'\n",
    "IDcol = 'Shopid'\n",
    "hourscol = 'hours'\n",
    "all_predictors, labelsvals, demfeats_vals = load_data_all(univ_type='CUMC')\n",
    "predictor_keys = get_predictor(keys)\n",
    "demo_predictor_keys = get_dem_predictors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = np.array(predictor_keys)\n",
    "continuous_cols = np.append(continuous_cols, np.array(demo_predictor_keys)[[0, 5]])\n",
    "categorical_cols = np.array(demo_predictor_keys)[[1,2,3,4]]\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "start_time = 24*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = uml.get_classifiers_list(cv)\n",
    "results_file_name = 'results'\n",
    "results_hours = train_models(all_predictors, demfeats_vals, labelsvals, continuous_cols, categorical_cols, all_classifiers,results_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchortimepoint = 168 # since we have 14*24 = 336 hours data, anchor will be at 336/2 = 168\n",
    "# for numofhours in np.arange(12, 336+12, 12):  # -156,156,24 - that ways -12 to 12 will be the full day with DCI\n",
    "range_idx = np.arange(start_time, 168+12, 12) # TODO: WE ARE CREATING MODELS GOING FORWARD STARTING FROM DAY=3,24*3 hrs\n",
    "for numofhours in range_idx:  # -156,156,24 - that ways -12 to 12 will be the full day with DCI\n",
    "    print(numofhours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_input(all_predictors, start_time, IDcol, predictor_keys, demfeats_vals, labelsvals):\n",
    "    anchortimepoint = 168   \n",
    "    results_hours = dict()\n",
    "    df_filtered = all_predictors[(all_predictors.hours >= start_time) & (\n",
    "            all_predictors.hours   <= numofhours)]\n",
    "    featurevals = df_filtered[[IDcol] + predictor_keys].groupby(IDcol).agg(\n",
    "        [uml.maxminRange_, np.nanmean, np.nanstd, np.nanmedian, uml.IQR_Range(25, 75),\n",
    "         uml.entropy_])\n",
    "    all_predictors.keys().T\n",
    "    all_predictors.shape\n",
    "    featurevals = pd.concat([featurevals, demfeats_vals], join='inner', axis=1)\n",
    "    featurevals = featurevals.dropna(axis=0, thresh=35)\n",
    "    labelsvals_hours = labelsvals[labelsvals.index.isin(featurevals.index)]\n",
    "    featurevals = featurevals.astype(float)\n",
    "    return featurevals, labelsvals_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(            Mean_HR                                                        \\\n",
       "        maxminRange_    nanmean     nanstd  nanmedian  IQR_Range  entropy_   \n",
       " Shopid                                                                      \n",
       " 2003.0    18.950000  67.994693   3.979893  67.983333   5.850000  4.288769   \n",
       " 2007.0    33.658333  72.295263   8.360221  72.195833  11.679167  4.283788   \n",
       " 2010.0    57.245833  83.411141  10.337122  84.264815  15.122917  4.135845   \n",
       " 2011.0    30.039583  72.467361   7.569291  70.377083  11.256250  3.395947   \n",
       " 2014.0    22.554167  81.139977   4.791501  81.354167   6.275000  4.288748   \n",
       " ...             ...        ...        ...        ...        ...       ...   \n",
       " 2675.0    43.712500  70.018881   8.626826  68.989583  11.339583  4.283081   \n",
       " 2677.0    31.177083  78.024612   7.528797  76.233333   9.245833  4.285947   \n",
       " 2681.0    34.155208  87.672007   6.980357  86.772917  10.810417  4.287368   \n",
       " 2682.0    45.718750  96.933735  13.693641  95.587500  31.669792  4.280634   \n",
       " 2688.0    26.529167  82.558092   6.066111  81.152083   8.002083  4.287831   \n",
       " \n",
       "              STD_HR                                ...    STD_RR            \\\n",
       "        maxminRange_   nanmean    nanstd nanmedian  ...    nanstd nanmedian   \n",
       " Shopid                                             ...                       \n",
       " 2003.0     3.199063  1.539206  0.616956  1.372020  ...       NaN       NaN   \n",
       " 2007.0     9.913622  5.221714  2.764282  4.830574  ...  1.135703  2.822907   \n",
       " 2010.0     4.838407  2.150551  1.109145  1.864191  ...  1.453390  1.886401   \n",
       " 2011.0     3.326867  2.912618  0.833315  2.602495  ...  1.141036  1.921896   \n",
       " 2014.0     3.349640  2.655551  0.748245  2.674925  ...  1.295351  4.987301   \n",
       " ...             ...       ...       ...       ...  ...       ...       ...   \n",
       " 2675.0     7.438973  4.443740  1.799127  4.483725  ...  0.859900  2.194996   \n",
       " 2677.0     5.253355  2.795236  0.999034  2.872477  ...  0.764730  2.173197   \n",
       " 2681.0     3.414879  1.780401  0.747617  1.593610  ...  0.524245  1.272545   \n",
       " 2682.0     5.963470  4.059431  1.095960  4.135480  ...  0.471372  1.316544   \n",
       " 2688.0     4.667815  2.180501  1.034543  1.949965  ...  0.349946  1.336964   \n",
       " \n",
       "                            Mean_CONTINUOUS_Age Mean_CATEGORICAL_Sex  \\\n",
       "        IQR_Range  entropy_           nanmedian            nanmedian   \n",
       " Shopid                                                                \n",
       " 2003.0       NaN  0.000000           53.167319                  1.0   \n",
       " 2007.0  1.531656  4.209801           57.153775                  1.0   \n",
       " 2010.0  1.619727  3.878369           40.759897                  1.0   \n",
       " 2011.0  2.368134  3.264980           34.741069                  1.0   \n",
       " 2014.0  1.647649  4.250646           45.025978                  1.0   \n",
       " ...          ...       ...                 ...                  ...   \n",
       " 2675.0  1.205210  4.208319           28.292296                  1.0   \n",
       " 2677.0  0.862854  4.224285           59.713763                  1.0   \n",
       " 2681.0  0.509671  4.203656           64.660632                  1.0   \n",
       " 2682.0  0.891964  4.230117           20.594566                  1.0   \n",
       " 2688.0  0.419279  4.258068           72.170757                  0.0   \n",
       " \n",
       "        Mean_CONTINUOUS_HH Mean_CONTINUOUS_MFS Mean_CONTINUOUS_WFNS  \\\n",
       "                 nanmedian           nanmedian            nanmedian   \n",
       " Shopid                                                               \n",
       " 2003.0                1.0                 2.0                  1.0   \n",
       " 2007.0                5.0                 2.0                  5.0   \n",
       " 2010.0                4.0                 2.0                  4.0   \n",
       " 2011.0                1.0                 0.0                  1.0   \n",
       " 2014.0                2.0                 0.0                  1.0   \n",
       " ...                   ...                 ...                  ...   \n",
       " 2675.0                3.0                 1.0                  4.0   \n",
       " 2677.0                2.0                 0.0                  2.0   \n",
       " 2681.0                3.0                 0.0                  2.0   \n",
       " 2682.0                3.0                 0.0                  2.0   \n",
       " 2688.0                2.0                 0.0                  1.0   \n",
       " \n",
       "        Mean_CONTINUOUS_GCS  \n",
       "                  nanmedian  \n",
       " Shopid                      \n",
       " 2003.0                15.0  \n",
       " 2007.0                 3.0  \n",
       " 2010.0                 8.0  \n",
       " 2011.0                15.0  \n",
       " 2014.0                15.0  \n",
       " ...                    ...  \n",
       " 2675.0                 8.0  \n",
       " 2677.0                14.0  \n",
       " 2681.0                14.0  \n",
       " 2682.0                14.0  \n",
       " 2688.0                15.0  \n",
       " \n",
       " [297 rows x 138 columns],\n",
       "         label\n",
       " Shopid       \n",
       " 2003.0    0.0\n",
       " 2007.0    1.0\n",
       " 2010.0    1.0\n",
       " 2011.0    0.0\n",
       " 2014.0    0.0\n",
       " ...       ...\n",
       " 2675.0    0.0\n",
       " 2677.0    0.0\n",
       " 2681.0    1.0\n",
       " 2682.0    0.0\n",
       " 2688.0    0.0\n",
       " \n",
       " [297 rows x 1 columns])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_data_input(all_predictors, start_time, IDcol, predictor_keys, demfeats_vals, labelsvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #********************1 . Define Parameters***********************\n",
    "    nanzscore = lambda x: (x - np.nanmean(x, axis=0)) / np.nanstd(x, axis=0)\n",
    "    randomstate = 7 # for reproducing results\n",
    "    include_dems = True\n",
    "    # anchordir = '/media/sf_Murad/Pickles_HR_SPO2/anchor/CUMC/'\n",
    "    for univ in ['CUMC','Aachen','UTH','all']: # create separate models for each university, and also with the combined data\n",
    "        # anchordir = '/mnt/H/Murad/Pickles_HR_SPO2/anchor/CUMC/'\n",
    "        # if not univ in ['all']:\n",
    "        #     continue\n",
    "        # if univ in ['all']:\n",
    "        #     load_data_all()\n",
    "        anchordir = '/mnt/H/Murad/Pickles_HR_SPO2/anchor/'+univ+'/'\n",
    "        keys = ['HR', 'AR-M', 'AR-D', 'AR-S', 'SPO2', 'RR']\n",
    "        target = 'label'\n",
    "        IDcol = 'Shopid'\n",
    "        hourscol = 'hours'\n",
    "        all_predictors, labelsvals, demfeats_vals = load_data_all(univ_type='all')\n",
    "        ## load all the pickles and append it in one array\n",
    "        predictor_keys = get_predictor(keys)\n",
    "        demo_predictor_keys = get_dem_predictors()\n",
    "        # if include_dems:\n",
    "        #     # get all the predictor keys\n",
    "        #     all_predictor_keys = predictor_keys + demo_predictor_keys\n",
    "        # else:\n",
    "        #     all_predictor_keys = predictor_keys\n",
    "        #\n",
    "        #\n",
    "        # # ***********************2 . Get all the predictors*******************\n",
    "        # all_predictors = get_predictor_data(anchordir,all_predictor_keys)\n",
    "        # all_predictors = all_predictors.astype(float)\n",
    "        # temp = all_predictors.groupby(['Shopid']).ffill()  # forward fill the missing values\n",
    "        # all_predictors[temp.keys()]=temp\n",
    "        #\n",
    "        # #NO NEED TO AGGR WE CAN TAKE UNIQUE TOO\n",
    "        # labelsvals = all_predictors[[IDcol, target]].groupby(IDcol).aggregate(np.nanmedian)  # get median and standard deviation\n",
    "        #\n",
    "        # # shopid = all_predictors[[IDcol, target]].groupby(IDcol).groups.keys()\n",
    "        #\n",
    "        # if include_dems:\n",
    "        #     demfeats_vals = all_predictors[[IDcol] + demo_predictor_keys].groupby(IDcol).aggregate(\n",
    "        #         [np.nanmedian])  # get median and standard deviation\n",
    "        # else:\n",
    "        #     demfeats_vals = []\n",
    "        #\n",
    "        # ## identify continous columns vs categorical columns for normalization during ML algorithms running\n",
    "        continuous_cols = np.array(predictor_keys)\n",
    "        if include_dems:\n",
    "            continuous_cols = np.append(continuous_cols, np.array(demo_predictor_keys)[[0, 5]])\n",
    "            categorical_cols = np.array(demo_predictor_keys)[[1,2,3,4]]\n",
    "        else:\n",
    "            categorical_cols = []\n",
    "\n",
    "        # **********************3. Get all the classifiers********************\n",
    "        cv = StratifiedKFold(n_splits=5)\n",
    "        if univ == 'UTH':\n",
    "            cv = StratifiedKFold(n_splits=3) # few patients, so 5 fold nested will fail\n",
    "        going_back_from_dci = True\n",
    "        if going_back_from_dci:\n",
    "            if include_dems:\n",
    "                result_dir = \"results_NO_ICP_TEMP_strt_dci/\"+univ+\"/\"\n",
    "            else:\n",
    "                result_dir = \"results_NO_ICP_TEMP_strt_dci_no_dem/\"+univ+\"/\"\n",
    "        else:\n",
    "            if include_dems:\n",
    "                result_dir = \"results_NO_ICP_TEMP/\"+univ+\"/\"\n",
    "            else:\n",
    "                result_dir = \"results_NO_ICP_TEMP_no_dem/\"+univ+\"/\"\n",
    "        if not os.path.isdir(result_dir):\n",
    "            os.mkdir(result_dir)\n",
    "        k_feat=70\n",
    "        start_time = 24*4 # starting form 3 days before DCI for going forward instead of the first time point as we have more data since PBD 3\n",
    "        for k_feat in [70]:#np.arange(20,80,10):\n",
    "            print(k_feat)\n",
    "            if not os.path.isdir(result_dir+str(k_feat)):\n",
    "                os.mkdir(result_dir+str(k_feat))\n",
    "            if not os.path.isdir(result_dir + str(k_feat)+\"/figures\"):\n",
    "                os.mkdir(result_dir + str(k_feat)+\"/figures\")\n",
    "            ## *****************************4. TRAIN CLASSIFIERS **************************************\n",
    "            all_classifiers = uml.get_classifiers_list(cv)\n",
    "            results_file_name = result_dir+str(k_feat)+\"/\"\n",
    "            results_hours = train_models(all_predictors, demfeats_vals, labelsvals, continuous_cols, categorical_cols, all_classifiers,results_file_name=results_file_name,k_feat=k_feat,\n",
    "                         include_dems=include_dems,going_back_from_dci=going_back_from_dci,start_time=start_time)\n",
    "\n",
    "            ## *********************5. SAVE RESULTS*********************\n",
    "            pickle.dump(results_hours, open(result_dir+str(k_feat)+\"/ML_Results_cumulative_one_min_RR_\"+str(k_feat)+\".p\", \"wb\"))\n",
    "            ## ************************* write Results *****************\n",
    "            write_results_csv(results_hours, result_dir+str(k_feat)+'/ML_Results_cumulative_one_min_RR_'+str(k_feat)+'.csv',going_back_from_dci=going_back_from_dci,start_time=start_time,name = 'EC')\n",
    "\n",
    "        ##TODO: Create Deep learning based classifer\n",
    "        nn_classifier = uml.get_NN_classifier(cv)\n",
    "        results_file_name = result_dir + 'NN' + \"/\"\n",
    "        if not os.path.isdir(result_dir + 'NN'):\n",
    "            os.mkdir(result_dir + 'NN')\n",
    "        if not os.path.isdir(result_dir + 'NN' + \"/figures\"):\n",
    "            os.mkdir(result_dir + 'NN'+ \"/figures\")\n",
    "        results_hours = train_models_NN(all_predictors, demfeats_vals, labelsvals, continuous_cols, categorical_cols,\n",
    "                                     None, results_file_name=results_file_name, k_feat=k_feat,\n",
    "                                     include_dems=include_dems, going_back_from_dci=going_back_from_dci,\n",
    "                                     start_time=start_time)\n",
    "\n",
    "        ## *********************5. SAVE RESULTS*********************\n",
    "        pickle.dump(results_hours,\n",
    "                    open(result_dir + 'NN' + \"/ML_Results_cumulative_one_min_RR_\" + str(k_feat) + \".p\", \"wb\"))\n",
    "        ## ************************* write Results *****************\n",
    "        write_results_csv(results_hours,\n",
    "                          result_dir + 'NN' + '/ML_Results_cumulative_one_min_RR_' + str(k_feat) + '.csv',\n",
    "                          going_back_from_dci=going_back_from_dci, start_time=start_time,)\n",
    "\n",
    "        #**********************6. Plot and Save Figures*****************\n",
    "        # for k_feat in np.arange(70, 80, 10):\n",
    "        for k_feat in np.arange(20, 80, 10):\n",
    "            xticklabelsvals = xtick_label_vals_feats(include_dems=True)\n",
    "            fin = open(\n",
    "                result_dir + str(k_feat) + \"/ML_Results_cumulative_one_min_RR_\" + str(k_feat) + \".p\", 'rb')\n",
    "            results_hours = pickle.load(fin, encoding='latin1')\n",
    "            fin.close()\n",
    "            plot_results(results_hours,xticklabelsvals,all_predictors,result_dir + str(k_feat)+'/figures/ML_Results_cumulative_one_min_RR'+str(k_feat))\n",
    "            plt.close('all')\n",
    "\n",
    "        ####  a. Plot Risk Scores for different models\n",
    "        for color, name in [\n",
    "            ('b', 'EC'),\n",
    "            # ('g', 'SL'),\n",
    "            # ('r', 'LR'),\n",
    "            # ('c', 'SK'),\n",
    "            # ('m', 'RF')\n",
    "        ]:\n",
    "            risk_score_file_name=result_dir + str(70) + \"/riskscores_\"+name+\".p\"\n",
    "            if os.path.isfile(risk_score_file_name):\n",
    "                print(risk_score_file_name)\n",
    "                plot_risk_scores(risk_score_file_name,name)\n",
    "                break\n",
    "\n",
    "        ####  b. Plot Risk Scores for EC classifier at different times\n",
    "        name = \"EC\"\n",
    "        name = \"NN\"\n",
    "        # results_file_name = result_dir + str(70)+\"/\"\n",
    "        results_file_name = result_dir + 'NN' + \"/\"\n",
    "        for numofhours in np.arange(start_time, 168+12, 12):\n",
    "            if numofhours >= 48:\n",
    "                risk_score_file_name = results_file_name + \"riskscores_\"+name +\"_\"+str(numofhours)+ \"__hourly.p\"\n",
    "                if os.path.isfile(risk_score_file_name):\n",
    "                    print(risk_score_file_name)\n",
    "                    plot_risk_scores(risk_score_file_name, name,vmin=0.25,vmax=0.8)\n",
    "                    plt.savefig(risk_score_file_name[0:-2] + '_line_plot' '.png')\n",
    "                    plt.close()\n",
    "                    plt.savefig(risk_score_file_name[0:-2] + '_images' '.png')\n",
    "                    plt.close()\n",
    "\n",
    "        ### c. plot correct discharges over time for different models\n",
    "        name=\"EC\"\n",
    "        results_file_name = result_dir + str(70)+\"/\"\n",
    "        for numofhours in np.arange(0, 168+12, 12):\n",
    "            if numofhours >= 48:\n",
    "                risk_score_file_name = results_file_name + \"riskscores_\"+name +\"_\"+str(numofhours)+ \"__hourly.p\"\n",
    "                if os.path.isfile(risk_score_file_name):\n",
    "                    print(risk_score_file_name)\n",
    "                    plot_tpv_fpv_over_time(risk_score_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
